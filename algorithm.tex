\section{Variable-Fidelity Model Learning Algorithm}
%
This section describes our learning algorithm for model generation,
shown in
algorithm~\ref{alg:compose_model}. This is a high-level description,
and the algorithm uses a diagnostic engine similar to the one
described by \cite{feldman13genius}.
%
\begin{algorithm}[htb]
\begin{footnotesize}
%
\caption{\textsc{ComposeModel}($G, \mathcal{C}, \mathcal{A}$)}
\label{alg:compose_model}
%
\KwIn{$G$, model topology}
\KwIn{$\mathcal{C}$, component library}
\KwIn{$\mathcal{A}$, set of test scenarios}
\KwResult{\sd, model}
%
\SetKwInput{KwLocals}{Local variables}
\SetKwInput{KwLocal}{Local variable}
\KwLocals{$\sd^\star$, $\sd^\prime$, models, initially $\emptyset$}
\KwLocal{$\alpha$, test scenario}
\KwLocal{$\omega$, diagnosis}
\KwLocal{$\gamma$, diagnosis score}
\KwLocal{$\gamma_{\min}$, optimal diagnosis score, initially $\infty$}
%
\vspace{0.075in}
%
\Repeat
{
$\textsc{Terminate?($\sd^\star, \sd^\prime, m$)}$
}
{
    $\sd^\star \gets \textsc{NextModelComposition}(G, \mathcal{C}, \sd^\prime)$\label{alg:next_model_composition}\\
    $\sd^\prime \gets \sd^\star$\\
    $m \gets 0$\\
    \ForEach{$\alpha \in \mathcal{A}$}
    {
        $\omega \gets \textsc{Diagnose}(\sd^\star, \alpha)$\label{alg:diagnose}\\
        $\gamma \gets \gamma + \textsc{Evaluate}(\alpha, \omega)$\label{alg:evaluate}\\
    }
    \If{$\gamma < \gamma_{\min}$\label{alg:accept_start}}
    {
        $\gamma_{\min} \gets \gamma$\\
        $\sd \gets \sd^\star$\label{alg:accept_end}\\
    }
}
\textbf{return} $\sd$
%
\end{footnotesize}
\end{algorithm}
%
\par
%
Algorithm~\ref{alg:compose_model} is non-deterministic. The
non-determinism is in the auxiliary function
\textsc{NextModelComposition}
(line~\ref{alg:next_model_composition}). This function takes a model
topology and a component library as inputs and returns a composed
model. Each component has multiple representations in the component
library (e.g., qualitative, linear, non-linear). The total number of
component combinations is $O(n^{|\comps|})$, where $n$ is the
number of representations. Fortunately, there is no need to perform a
complete search over the space of all possible model compositions. A
greedy-search strategy achieves satisfactory performance in most
practical cases.
\par
The subroutine \textsc{Diagnose} in line \ref{alg:diagnose} implements
a diagnostic oracle. Given a system description $\sd^\star$, and an
observation $\alpha$, it computes a diagnosis $\omega$ (see
definition~\ref{def:diagnosis}). This diagnosis can be compared to the
fault injection to compute one or more diagnostic metrics. This is
done by the \textsc{Evaluate} subroutine in line
\ref{alg:evaluate}. The combined metric result is accumulated in a variable
$m$. Algorithm \ref{alg:compose_model} assumes that larger metrics are
worse, e.g. metric results are penalties, and it chooses the model
that minimizes the cumulative penalty. The assumption is that the set
of test scenarios is representative and the learned model $\sd$ is
going to minimize future scenarios with unknown faults.
\par
Consider again the three-tanks example. The subroutine
\textsc{NextModelComposition} first generates a model where $T_1$ is
non-linear, $T_2$ is non-linear, and $T_3$ is non-linear. In the second
call \textsc{NextModelComposition} changes $T_1$ from non-linear to
linear. Depending on the implementation of
\textsc{NextModelComposition} the next candidate can be either $T_1$
and $T_2$ non-linear while $T_3$ linear or $T_1$ non-linear, $T_2$
linear, and $T_3$ non-linear.
\par
In our implementation we provide the following model-composition
search policies:
%
\begin{description}
%
\item[Breadth-First Search (BFS):]
{
%
This search policy starts with all components having the same model
types (for example non-linear), then considers all models with a
single component type change. After all single component type changes
are exhausted, the algorithm continues with pairs of components,
then triples, etc.
%
}
\item[Depth-First Search (DFS):]
{
%
The algorithm starts by changing the type of the first component, then
the second, etc., until all component types are changed. At this
point, algorithm~\ref{alg:compose_model} backtracks one step,
generates a sibling assignment and continues traversing down and
backtracking in the same manner until no more backtracking is
possible.
%
}
\item[Forward Greedy Stochastic Search (FGSS):]
{
%
This is a randomized search policy. In this mode, the algorithm starts
by changing the type of one of the components. If the change improves
the metric in line~\ref{alg:evaluate} of
algorithm~\ref{alg:compose_model}, then the change is accepted (see
lines~\ref{alg:accept_start}--\ref{alg:accept_end} of
algorithm~\ref{alg:compose_model}). This is our preferred search
policy as typically the evaluation metric improves monotonically when
changing the component types one by one.
%
}
\item[Backwards Greedy Stochastic Search (BGSS):]
{
%
In this mode, the search starts with all component types changed from
their defaults. The type of a random component is then flipped and the
flip is retained iff the flip leads to a decrease in the total metric
evaluation score. The order of components is arbitrary. As the whole
search process is stochastic, it needs to be run multiple iterations
are necessary in order to achieve the desired completeness.
%
}
%
\end{description}
\par
The computational performance of algorithm~\ref{alg:compose_model} is
determined by the search choice, given an efficient implementation of
the \textsc{Diagnose} subroutine:
%
\begin{proposition}\label{theorem:complexity}
%
Algorithm~\ref{alg:compose_model} will terminate in $O(m^n)$ calls to
a diagnostic oracle, given exhaustive search (BFS and DFS) and in
$O(n)$ calls to a diagnostic oracle given a greedy search (FGSS or
BGSS).
%
\end{proposition}
%
The proof of proposition~\ref{theorem:complexity} is elementary from
the number of combinations of component type in each component
ensemble.
\par
Like any learning method, algorithm~\ref{alg:compose_model} assumes
that the future is predictable given a test set of diagnostic
scenarios. The model computed by algorithm~\ref{alg:compose_model}
will optimize the cumulative metrics for this past set of scenarios
but there is, of course, no guarantee that the model is optimal in the
general case.
