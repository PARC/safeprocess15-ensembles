%************************************************************************
\section{Related Work}
\label{sec-RelWork}
%************************************************************************
Model ensemble methods have been applied in disciplines ranging from statistics
to AI (e.g.,~\citep{breiman1996stacked,clemen1989combining,wolpert1992stacked}),
and it has been shown that an ensemble is often more accurate than any single
model in the ensemble~\citep{maclin2011popular}.

The motivation behind using a model ensemble is to build a predictive
model by integrating multiple models to achieve better performance
than could be obtained from using models
individually~\citep{maclin2011popular,rokach2010ensemble}. An ensemble
method is a supervised learning-based approach to discover which
combination of model has the best predictive
power~\citep{kuncheva2003measures}.

The most popular ensemble algorithms are Bagging and Boosting, which are meta-algorithms
that pool decisions from multiple classifiers. Both these algorithms apply the Mixture
of Experts paradigm~\citep{brown2010ensemble}.
%
The method proposed in 1996 by \cite{breiman1996stacked} stands for
Bootstrap Aggregating, is a \textit{bootstrap}-based ensemble
method~\citep{efron1994introduction}.  It applies a majority vote from
classifiers trained on bootstrap samples (obtained by random sampling
with replacement) of the training data.

Boosting~\citep{freund1996experiments,schapire1990strength} is an
iteratively learning algorithm that produces a weighted sum of the
results of weak classifiers. The training set used for each member of
the series is chosen based on the performance of the earlier
classifiers in the series. Note that, conversely to Boosting, the
resampling of the training set in Bagging is not dependent on the
performance of the earlier classifiers.  Many kinds of boosting
algorithms have been proposed in the past: the award winning
Adaboost~\citep{freund1996experiments}; LPBoost
\citep{demiriz2002linear}: a margin-maximizing classification
algorithm with boosting; BrownBoost~\cite{freund2001adaptive}: a
boosting algorithm that may be robust to noisy datasets; and
LogitBoost~\citep{friedman2000additive}: an approach which casts the
AdaBoost algorithm into a statistical framework.

To the best of our knowledge, model ensemble methods have never been
used for diagnosis.

In the context of software fault localization using spectrum-based
fault localization~\citep{abreu2007accuracy}, approaches have been
proposed to combine several heuristics. In~\citep{wang2011search} a
search-based algorithm to combine the existing
heuristics. \cite{xuan2014learning} proposed a machine learning-based
approach to combine multiple diagnostic rankings metrics. An approach
based in data fusion to heuristic combination was proposed
in~\citep{lo2014fusion}.

% @@@ Alex: Next paragraph is commented out to fit in 6 pp.
%Another body of related work is research related to choosing the right level of
%modeling abstraction. As an example, in the model-based diagnosis literature
%there has been considerable work on diagnostic assumptions and selecting appropriate
%models for a diagnostic task~\citep{struss1992s}. The work proposed in~\citep{de2007dynamic}
%focuses primarily on assumptions associated with choosing domain abstractions.

There has been considerable research on structural
abstraction~\citep{chittaro2004hierarchical,hamscher1990xde} where
groups of components are combined to form larger systems to reduce
computational complexity. The work in~\citep{sachenbacher2005task}
describes how the task can be used to partition the value of a
variable into the qualitative values needed to solve a
task. In~\citep{torta2003automatic} presents another approach to
partition the value of a variable into qualitative ranges to reduce
complexity when there is limited observability of the variables.
